# 数据库迁移指南：SQLite + ChromaDB → PostgreSQL + pgvector

> **文档创建日期**: 2025-11-25
> **迁移状态**: ✅ 文档已更新，待代码实施

---

## 1. 迁移概述

### 1.1 迁移目标

将Personal Growth OS的数据存储方案从本地 **SQLite + ChromaDB** 迁移到云端 **PostgreSQL + pgvector**。

### 1.2 迁移原因

| 当前方案 | 痛点 | 新方案优势 |
|---------|------|-----------|
| SQLite (本地文件) | 无法多设备同步 | PostgreSQL (云端) 支持远程访问 |
| ChromaDB (独立向量库) | 数据一致性难保证 | pgvector (PG扩展) ACID事务保证一致性 |
| 两个数据源 | 部署复杂，备份困难 | 单一数据库，统一管理 |
| 本地存储 | 数据丢失风险高 | 云端自动备份，高可用 |

### 1.3 迁移范围

**已更新的文档**:
- ✅ `doc/系统现状总结.md` - 技术栈和架构图
- ✅ `doc/后端详细设计.md` - 数据库架构章节
- ✅ `doc/RAG系统详细设计.md` - 向量存储方案
- ✅ `CLAUDE.md` - 开发指南和配置
- ✅ `README.md` - 快速开始指南
- ✅ `.env.example` - 环境配置模板

**待实施的代码变更**:
- ⏸️ `backend/app/db/database.py` - 数据库连接配置
- ⏸️ `backend/app/db/models.py` - 数据模型迁移
- ⏸️ `backend/app/services/vector_store.py` - 向量存储服务重写
- ⏸️ `backend/requirements.txt` - 依赖更新
- ⏸️ 数据库迁移脚本 (Alembic)

---

## 2. 新架构说明

### 2.1 PostgreSQL表结构

```sql
-- 1. 关系型表 (已有，需调整类型)
CREATE TABLE tasks (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    description TEXT,
    status VARCHAR(20) DEFAULT 'pending',
    priority INTEGER CHECK (priority BETWEEN 1 AND 5),
    due_date TIMESTAMP WITH TIME ZONE,
    snooze_until TIMESTAMP WITH TIME ZONE,
    start_time TIMESTAMP WITH TIME ZONE,
    end_time TIMESTAMP WITH TIME ZONE,
    estimated_hours FLOAT,
    project_id INTEGER REFERENCES projects(id),
    parent_task_id INTEGER REFERENCES tasks(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE notes (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    content TEXT,
    source_url TEXT,
    project_id INTEGER REFERENCES projects(id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE projects (
    id SERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL,
    description TEXT,
    color VARCHAR(7),
    is_system BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 2. 向量表 (新增)
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE note_embeddings (
    id SERIAL PRIMARY KEY,
    note_id INTEGER REFERENCES notes(id) ON DELETE CASCADE UNIQUE,
    chunk_index INTEGER DEFAULT 0,
    content TEXT,
    embedding vector(384),  -- all-MiniLM-L6-v2 维度
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 3. 向量索引
CREATE INDEX note_embeddings_embedding_idx
  ON note_embeddings
  USING ivfflat (embedding vector_cosine_ops)
  WITH (lists = 100);

CREATE INDEX note_embeddings_note_id_idx
  ON note_embeddings(note_id);
```

### 2.2 语义搜索查询示例

```sql
-- 查找最相似的笔记 (余弦相似度)
SELECT
    n.id,
    n.title,
    n.content,
    1 - (ne.embedding <=> :query_vector) AS similarity
FROM notes n
JOIN note_embeddings ne ON n.id = ne.note_id
ORDER BY ne.embedding <=> :query_vector
LIMIT 5;
```

---

## 3. 环境配置

### 3.1 PostgreSQL配置

**`.env` 文件** (backend/.env):
```env
# PostgreSQL数据库
DATABASE_URL=postgresql://username:password@your-cloud-db-host:5432/personal_growth_os

# Embedding模型
EMBEDDING_MODEL=all-MiniLM-L6-v2

# LLM配置 (保持不变)
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

### 3.2 依赖更新

**`requirements.txt` 需要调整**:

```diff
# 数据库
sqlalchemy==2.0.36
-alembic==1.14.0
+alembic==1.14.0
+psycopg2-binary==2.9.9  # PostgreSQL驱动
+pgvector==0.3.6         # pgvector Python客户端

# 向量相关
-chromadb>=0.5.0         # 移除ChromaDB
+sentence-transformers>=2.2.0  # 本地Embedding模型
```

---

## 4. 代码迁移清单

### 4.1 数据库连接 (`backend/app/db/database.py`)

**修改前 (SQLite)**:
```python
SQLALCHEMY_DATABASE_URL = "sqlite:///./personal_growth_os.db"
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args={"check_same_thread": False}
)
```

**修改后 (PostgreSQL)**:
```python
from app.core.config import settings

SQLALCHEMY_DATABASE_URL = settings.DATABASE_URL
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)
```

### 4.2 数据模型 (`backend/app/db/models.py`)

**关键变更**:
1. `Integer` → `SERIAL` (自增ID自动处理)
2. `DateTime` → `TIMESTAMP WITH TIME ZONE`
3. 新增 `NoteEmbedding` 模型

```python
from sqlalchemy import Column, Integer, String, Text, Float, Boolean, ForeignKey, TIMESTAMP
from sqlalchemy.dialects.postgresql import ARRAY
from pgvector.sqlalchemy import Vector

class NoteEmbedding(Base):
    __tablename__ = "note_embeddings"

    id = Column(Integer, primary_key=True, index=True)
    note_id = Column(Integer, ForeignKey("notes.id", ondelete="CASCADE"), unique=True)
    chunk_index = Column(Integer, default=0)
    content = Column(Text)
    embedding = Column(Vector(384))  # all-MiniLM-L6-v2维度
    created_at = Column(TIMESTAMP(timezone=True), server_default=func.now())
    updated_at = Column(TIMESTAMP(timezone=True), onupdate=func.now())
```

### 4.3 向量存储服务 (`backend/app/services/vector_store.py`)

**完全重写**，从 ChromaDB 切换到 pgvector:

```python
from sentence_transformers import SentenceTransformer
from sqlalchemy import text
from app.db.database import SessionLocal
from app.db.models import NoteEmbedding, Note

class VectorStoreService:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def add_note_embedding(self, note_id: int, content: str):
        """将笔记向量化并存入数据库"""
        embedding = self.model.encode(content).tolist()

        db = SessionLocal()
        try:
            note_emb = NoteEmbedding(
                note_id=note_id,
                content=content,
                embedding=embedding
            )
            db.add(note_emb)
            db.commit()
        finally:
            db.close()

    def search_similar_notes(self, query: str, limit: int = 5):
        """语义搜索相似笔记"""
        query_embedding = self.model.encode(query).tolist()

        db = SessionLocal()
        try:
            sql = text("""
                SELECT n.id, n.title, n.content,
                       1 - (ne.embedding <=> :query_vec) AS similarity
                FROM notes n
                JOIN note_embeddings ne ON n.id = ne.note_id
                ORDER BY ne.embedding <=> :query_vec
                LIMIT :limit
            """)

            result = db.execute(
                sql,
                {"query_vec": query_embedding, "limit": limit}
            )
            return result.fetchall()
        finally:
            db.close()
```

### 4.4 Alembic迁移脚本

创建迁移脚本：
```bash
cd backend
alembic revision -m "migrate_to_postgresql_pgvector"
```

在生成的迁移文件中：
```python
def upgrade():
    # 1. 创建pgvector扩展
    op.execute('CREATE EXTENSION IF NOT EXISTS vector')

    # 2. 创建note_embeddings表
    op.create_table(
        'note_embeddings',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('note_id', sa.Integer(), sa.ForeignKey('notes.id', ondelete='CASCADE')),
        sa.Column('chunk_index', sa.Integer(), default=0),
        sa.Column('content', sa.Text()),
        sa.Column('embedding', Vector(384)),
        sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default=sa.func.now()),
        sa.Column('updated_at', sa.TIMESTAMP(timezone=True))
    )

    # 3. 创建索引
    op.execute("""
        CREATE INDEX note_embeddings_embedding_idx
        ON note_embeddings
        USING ivfflat (embedding vector_cosine_ops)
        WITH (lists = 100)
    """)
```

---

## 5. 数据迁移步骤

### 5.1 准备工作

1. **备份现有数据**:
   ```bash
   # SQLite备份
   cp backend/personal_growth_os.db backend/personal_growth_os.db.backup

   # ChromaDB备份
   cp -r backend/chroma_data backend/chroma_data.backup
   ```

2. **准备PostgreSQL环境**:
   - 确认云数据库可访问
   - 安装pgvector扩展
   - 创建数据库和用户

### 5.2 迁移执行

**方案1: 使用迁移脚本**（推荐）

创建 `backend/utils/migrate_to_pg.py`:
```python
import sqlite3
import psycopg2
from sentence_transformers import SentenceTransformer

def migrate_data():
    # 1. 连接SQLite和PostgreSQL
    sqlite_conn = sqlite3.connect('personal_growth_os.db')
    pg_conn = psycopg2.connect(os.getenv('DATABASE_URL'))

    # 2. 迁移关系数据
    migrate_projects(sqlite_conn, pg_conn)
    migrate_tasks(sqlite_conn, pg_conn)
    migrate_notes(sqlite_conn, pg_conn)
    migrate_tags(sqlite_conn, pg_conn)

    # 3. 迁移向量数据 (从ChromaDB)
    migrate_embeddings(pg_conn)

    sqlite_conn.close()
    pg_conn.close()

def migrate_embeddings(pg_conn):
    """从ChromaDB迁移embedding到pgvector"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    cursor = pg_conn.cursor()

    # 获取所有笔记
    cursor.execute("SELECT id, content FROM notes")
    notes = cursor.fetchall()

    for note_id, content in notes:
        # 重新生成embedding
        embedding = model.encode(content).tolist()

        # 插入pgvector
        cursor.execute("""
            INSERT INTO note_embeddings (note_id, content, embedding)
            VALUES (%s, %s, %s)
        """, (note_id, content, embedding))

    pg_conn.commit()
```

运行迁移：
```bash
cd backend
python utils/migrate_to_pg.py
```

**方案2: 手动导出导入**

```bash
# 1. 导出SQLite数据为SQL
sqlite3 personal_growth_os.db .dump > dump.sql

# 2. 转换SQL语法 (SQLite → PostgreSQL)
sed -i 's/INTEGER PRIMARY KEY AUTOINCREMENT/SERIAL PRIMARY KEY/g' dump.sql
sed -i 's/DATETIME/TIMESTAMP WITH TIME ZONE/g' dump.sql

# 3. 导入PostgreSQL
psql $DATABASE_URL < dump.sql
```

### 5.3 验证迁移

运行验证脚本 `backend/utils/verify_migration.py`:
```python
def verify_migration():
    db = SessionLocal()

    # 1. 检查数据量
    task_count = db.query(Task).count()
    note_count = db.query(Note).count()
    embedding_count = db.query(NoteEmbedding).count()

    print(f"Tasks: {task_count}")
    print(f"Notes: {note_count}")
    print(f"Embeddings: {embedding_count}")

    # 2. 测试向量搜索
    vector_service = VectorStoreService()
    results = vector_service.search_similar_notes("测试查询", limit=3)
    print(f"Search results: {len(results)}")

    db.close()
```

---

## 6. 回滚计划

如果迁移失败，可快速回滚：

```bash
# 1. 恢复backend/.env配置
cp backend/.env.backup backend/.env

# 2. 恢复SQLite数据库
cp backend/personal_growth_os.db.backup backend/personal_growth_os.db

# 3. 恢复ChromaDB
rm -rf backend/chroma_data
cp -r backend/chroma_data.backup backend/chroma_data

# 4. 重启服务
cd backend
python -m uvicorn app.main:app --reload
```

---

## 7. 性能优化建议

### 7.1 向量索引优化

```sql
-- 根据数据量选择索引类型
-- < 10k vectors: 使用IVFFlat, lists=100
-- 10k-100k vectors: IVFFlat, lists=200-500
-- > 100k vectors: 考虑HNSW (PG 15+)

-- 监控索引性能
EXPLAIN ANALYZE
SELECT * FROM note_embeddings
ORDER BY embedding <=> '[0.1, 0.2, ...]'::vector
LIMIT 5;
```

### 7.2 连接池配置

```python
# backend/app/db/database.py
engine = create_engine(
    DATABASE_URL,
    pool_size=20,          # 连接池大小
    max_overflow=10,       # 额外连接数
    pool_recycle=3600,     # 1小时回收连接
    pool_pre_ping=True     # 检测连接有效性
)
```

### 7.3 批量操作优化

```python
# 批量插入embedding
from sqlalchemy import insert

embeddings_batch = [
    {"note_id": 1, "embedding": [...], "content": "..."},
    {"note_id": 2, "embedding": [...], "content": "..."},
    # ...
]

stmt = insert(NoteEmbedding).values(embeddings_batch)
db.execute(stmt)
db.commit()
```

---

## 8. 监控和维护

### 8.1 定期维护任务

```bash
# 每周执行
psql $DATABASE_URL -c "VACUUM ANALYZE note_embeddings;"

# 每月执行
psql $DATABASE_URL -c "REINDEX TABLE note_embeddings;"
```

### 8.2 性能监控

```sql
-- 查看表大小
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 查看索引使用情况
SELECT
    indexrelname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public';
```

---

## 9. 常见问题

### Q1: 向量搜索性能不佳？
**A**: 检查索引是否正确创建，调整IVFFlat的lists参数。

### Q2: 迁移后数据丢失？
**A**: 检查外键约束，确保关联数据先迁移。

### Q3: Embedding维度不匹配？
**A**: 确认使用 all-MiniLM-L6-v2 模型 (384维)。

### Q4: 连接数过多导致超时？
**A**: 调整pool_size和max_overflow参数。

---

## 10. 参考资源

- [pgvector官方文档](https://github.com/pgvector/pgvector)
- [PostgreSQL性能优化](https://www.postgresql.org/docs/current/performance-tips.html)
- [SQLAlchemy迁移指南](https://docs.sqlalchemy.org/en/20/core/engines.html)
- [Sentence Transformers](https://www.sbert.net/)

---

**文档维护**: 迁移完成后请更新本文档的"迁移状态"
