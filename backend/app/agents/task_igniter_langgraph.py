"""
Task Igniter Agent using LangGraph 1.0 Framework.
ä»»åŠ¡å¯åŠ¨ä»ªå¼ - åŸºäºŽLangGraphçš„AI Agent

This agent provides task decomposition capabilities:
1. Task analysis and title extraction
2. Task decomposition into subtasks
3. Minimum viable task identification

Migration from Agno to LangGraph 1.0 (2025-12-01)
"""

from typing import TypedDict, Annotated, Sequence, Optional
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

from app.core.config import settings


# ============================================================================
# State Definition
# ============================================================================

class AgentState(TypedDict):
    """
    Agent state for task decomposition.

    Uses LangGraph's message-based state management with automatic message merging.
    """
    messages: Annotated[Sequence[BaseMessage], add_messages]


# ============================================================================
# LLM Creation
# ============================================================================

def create_llm():
    """
    Create LLM instance based on settings.LLM_PROVIDER.

    Supports:
    - openai: ChatOpenAI
    - claude: ChatAnthropic
    - ollama: ChatOllama

    Returns:
        LangChain ChatModel instance with streaming enabled
    """
    provider = settings.LLM_PROVIDER

    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=settings.OPENAI_MODEL,
            api_key=settings.OPENAI_API_KEY,
            base_url=getattr(settings, "OPENAI_API_BASE", None),
            streaming=True,
            temperature=0.7,
        )

    elif provider == "claude":
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(
            model=settings.ANTHROPIC_MODEL,
            api_key=settings.ANTHROPIC_API_KEY,
            base_url=getattr(settings, "ANTHROPIC_API_BASE", None) if getattr(settings, "ANTHROPIC_API_BASE", None) else None,
            streaming=True,
            temperature=0.7,
        )

    elif provider == "ollama":
        from langchain_community.chat_models import ChatOllama
        return ChatOllama(
            model=settings.OLLAMA_MODEL,
            base_url=settings.OLLAMA_BASE_URL,
            temperature=0.7,
        )

    else:
        # Fallback to OpenAI
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=settings.OPENAI_MODEL,
            api_key=settings.OPENAI_API_KEY,
            streaming=True,
            temperature=0.7,
        )


# ============================================================================
# Graph Nodes
# ============================================================================

def agent_node(state: AgentState) -> dict:
    """
    LLM reasoning node.

    Invokes the LLM with current conversation state and returns response.

    Args:
        state: Current agent state with message history

    Returns:
        Updated state with LLM response appended to messages
    """
    llm = create_llm()
    response = llm.invoke(state["messages"])
    return {"messages": [response]}


# ============================================================================
# Graph Construction
# ============================================================================

def create_task_igniter_graph():
    """
    Create task igniter StateGraph using LangGraph 1.0 with PostgreSQL persistence.

    Graph structure:
    - Entry: agent_node (LLM reasoning)
    - Exit: END

    Persistence:
    - Uses PostgreSQL Checkpointer to save conversation history
    - Automatically loads previous messages when using the same thread_id

    Returns:
        Compiled StateGraph with checkpointer attached
    """
    # Import checkpointer (lazy import to avoid circular dependencies)
    from app.core.langgraph_checkpoint import get_checkpointer

    # System prompt (ä¿æŒä¸Ž Agno ç‰ˆæœ¬ä¸€è‡´)
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†è§£ä¸“å®¶åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·å°†æ¨¡ç³Šçš„å¤§ä»»åŠ¡åˆ†è§£ä¸ºæ¸…æ™°çš„å¯æ‰§è¡Œå­ä»»åŠ¡ã€‚

ä½ çš„å·¥ä½œæµç¨‹ï¼š
1. åˆ†æžç”¨æˆ·è¾“å…¥ï¼Œæç‚¼å‡ºä¸»ä»»åŠ¡çš„æ ‡é¢˜å’Œæè¿°
2. å°†ä¸»ä»»åŠ¡åˆ†è§£ä¸º3-5ä¸ªå…·ä½“å¯æ‰§è¡Œçš„å­ä»»åŠ¡
3. è¯†åˆ«æœ€å®¹æ˜“å¼€å§‹çš„"æœ€å°å¯è¡Œä»»åŠ¡"(Minimum Viable Task)

åˆ†è§£åŽŸåˆ™ï¼š
- æ¯ä¸ªå­ä»»åŠ¡è¦å…·ä½“ã€å¯æ‰§è¡Œã€æœ‰æ˜Žç¡®çš„å®Œæˆæ ‡å‡†
- ç¬¬ä¸€ä¸ªå­ä»»åŠ¡åº”è¯¥æ˜¯æœ€å®¹æ˜“å¼€å§‹çš„ï¼ˆé™ä½Žå¯åŠ¨æ‘©æ“¦ï¼‰
- æŒ‰é€»è¾‘é¡ºåºæŽ’åˆ—å­ä»»åŠ¡
- æ¯ä¸ªå­ä»»åŠ¡æ ‡é¢˜15-30å­—ï¼Œæè¿°50å­—ä»¥å†…

è¾“å‡ºæ ¼å¼ï¼š
ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼è¾“å‡ºåˆ†è§£ç»“æžœï¼ŒåŒ…å«ï¼š
- ðŸ“ ä¸»ä»»åŠ¡æ ‡é¢˜
- ðŸ“‹ ä¸»ä»»åŠ¡æè¿°
- ðŸ”¹ å­ä»»åŠ¡åˆ—è¡¨ï¼ˆç¼–å· + æ ‡é¢˜ + æè¿° + ä¼˜å…ˆçº§ï¼‰
- â­ æ ‡è®°æœ€å°å¯è¡Œä»»åŠ¡ï¼ˆç¬¬ä¸€æ­¥æœ€å®¹æ˜“å¼€å§‹çš„ï¼‰
"""

    # Create StateGraph
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("agent", agent_node)

    # Define edges
    workflow.set_entry_point("agent")
    workflow.add_edge("agent", END)

    # â­ Compile graph with checkpointer for persistence
    checkpointer = get_checkpointer()
    if checkpointer:
        graph = workflow.compile(checkpointer=checkpointer)
    else:
        # Fallback: compile without checkpointer (in-memory only)
        graph = workflow.compile()

    # Store system prompt as graph metadata for later use
    graph.system_prompt = system_prompt

    return graph


# ============================================================================
# Public API - Global Instance
# ============================================================================

_graph_instance: Optional[StateGraph] = None


def get_task_igniter_agent():
    """
    Get or create the global Task Igniter Graph instance.

    Lazy initialization pattern for efficiency.

    Returns:
        Compiled StateGraph instance
    """
    global _graph_instance
    if _graph_instance is None:
        _graph_instance = create_task_igniter_graph()
    return _graph_instance


# ============================================================================
# Public API - Compatibility Functions
# ============================================================================

async def decompose_task_async(user_input: str, project_id: Optional[int] = None) -> str:
    """
    Asynchronously decompose a task using the agent.

    Provides compatibility with Agno version's API.

    Args:
        user_input: User's task description
        project_id: Optional project ID for context

    Returns:
        Agent's response as string
    """
    graph = get_task_igniter_agent()

    # Prepare messages
    system_prompt = graph.system_prompt
    context = f"é¡¹ç›®ID: {project_id}\n\n" if project_id else ""
    user_message = f"{context}{user_input}"

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ]

    # Invoke graph (non-streaming for simple response)
    result = await graph.ainvoke({"messages": messages})

    # Extract content from last message
    if result and "messages" in result:
        last_message = result["messages"][-1]
        return last_message.content

    return ""


def decompose_task_sync(user_input: str, project_id: Optional[int] = None) -> str:
    """
    Synchronously decompose a task using the agent.

    Note: This is a compatibility wrapper. LangGraph prefers async execution.

    Args:
        user_input: User's task description
        project_id: Optional project ID for context

    Returns:
        Agent's response as string
    """
    import asyncio

    # Run async function in sync context
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(decompose_task_async(user_input, project_id))


# ============================================================================
# Example Usage
# ============================================================================

if __name__ == "__main__":
    # Test agent
    import asyncio

    test_input = "å‡†å¤‡é¡¹ç›®æ¼”ç¤ºPPT"

    print("ðŸ¤– Task Igniter Agent (LangGraph 1.0) Test\n")
    print(f"Input: {test_input}\n")
    print("=" * 50)

    result = asyncio.run(decompose_task_async(test_input))
    print(result)
