"""
Task Igniter Agent using LangGraph 1.0 Framework.
ä»»åŠ¡å¯åŠ¨ä»ªå¼ - åŸºäºŽLangGraphçš„AI Agent

This agent provides task decomposition capabilities:
1. Task analysis and title extraction
2. Task decomposition into subtasks
3. Minimum viable task identification

Migration from Agno to LangGraph 1.0 (2025-12-01)
Updated 2025-12-03: Use create_react_agent for proper streaming support
"""

from typing import Optional, Any
from langchain_core.messages import HumanMessage

from app.core.config import settings


# ============================================================================
# System Prompt
# ============================================================================

SYSTEM_PROMPT = """ä½ æ˜¯ Personal Growth OS çš„ AI åŠ©æ‰‹ï¼Œä¸“æ³¨äºŽå¸®åŠ©ç”¨æˆ·æå‡æ•ˆçŽ‡å’Œä¸ªäººæˆé•¿ã€‚

ä½ çš„ä¸»è¦èƒ½åŠ›ï¼š
1. **ä»»åŠ¡åˆ†è§£**: å½“ç”¨æˆ·æè¿°ä¸€ä¸ªä»»åŠ¡æˆ–ç›®æ ‡æ—¶ï¼Œå¸®åŠ©åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„å­ä»»åŠ¡
2. **æ—¥å¸¸å¯¹è¯**: å‹å¥½åœ°å›žåº”ç”¨æˆ·çš„é—®å€™å’Œé—²èŠ
3. **é—®é¢˜è§£ç­”**: å›žç­”å…³äºŽä»»åŠ¡ç®¡ç†ã€æ—¶é—´è§„åˆ’ç­‰é—®é¢˜

åˆ¤æ–­è§„åˆ™ï¼š
- å¦‚æžœç”¨æˆ·è¾“å…¥æ˜¯é—®å€™ï¼ˆå¦‚"ä½ å¥½"ã€"hi"ã€"hello"ç­‰ï¼‰ï¼Œå‹å¥½åœ°å›žåº”å¹¶è¯¢é—®å¦‚ä½•å¸®åŠ©
- å¦‚æžœç”¨æˆ·æè¿°äº†ä¸€ä¸ªå…·ä½“ä»»åŠ¡æˆ–ç›®æ ‡ï¼Œè¿›è¡Œä»»åŠ¡åˆ†è§£
- å¦‚æžœä¸ç¡®å®šç”¨æˆ·æ„å›¾ï¼Œç¤¼è²Œåœ°è¯¢é—®

ä»»åŠ¡åˆ†è§£æ ¼å¼ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰ï¼š
ðŸ“ **ä¸»ä»»åŠ¡æ ‡é¢˜**
ðŸ“‹ ä¸»ä»»åŠ¡æè¿°

ðŸ”¹ **å­ä»»åŠ¡åˆ—è¡¨**
1. å­ä»»åŠ¡æ ‡é¢˜ - æè¿°ï¼ˆä¼˜å…ˆçº§ï¼šé«˜/ä¸­/ä½Žï¼‰â­æœ€å°å¯è¡Œä»»åŠ¡
2. å­ä»»åŠ¡æ ‡é¢˜ - æè¿°ï¼ˆä¼˜å…ˆçº§ï¼šé«˜/ä¸­/ä½Žï¼‰
...

åˆ†è§£åŽŸåˆ™ï¼š
- æ¯ä¸ªå­ä»»åŠ¡å…·ä½“ã€å¯æ‰§è¡Œã€æœ‰æ˜Žç¡®å®Œæˆæ ‡å‡†
- ç¬¬ä¸€ä¸ªå­ä»»åŠ¡åº”æœ€å®¹æ˜“å¼€å§‹ï¼ˆé™ä½Žå¯åŠ¨æ‘©æ“¦ï¼‰
- 3-5ä¸ªå­ä»»åŠ¡ä¸ºå®œ
"""


# ============================================================================
# LLM Creation
# ============================================================================

def create_llm():
    """
    Create LLM instance based on settings.LLM_PROVIDER.

    Supports:
    - openai: ChatOpenAI (with JWT token support)
    - claude: ChatAnthropic
    - ollama: ChatOllama

    Returns:
        LangChain ChatModel instance with streaming enabled

    Note:
        For OpenAI provider, automatically detects JWT tokens and uses
        Authorization header for proxy services (e.g., TrendMicro).
    """
    # Use centralized utility function with JWT auth support
    from app.core.llm_utils import get_langchain_llm_with_auth
    return get_langchain_llm_with_auth()


# ============================================================================
# Graph Construction using create_react_agent
# ============================================================================

def create_task_igniter_graph():
    """
    Create task igniter agent using LangGraph 1.0's create_react_agent.

    Uses create_react_agent for proper streaming support with astream_events.
    The agent uses a system prompt for task decomposition guidance.

    Persistence:
    - Uses PostgreSQL Checkpointer to save conversation history
    - Automatically loads previous messages when using the same thread_id

    Returns:
        Compiled agent graph with checkpointer attached
    """
    import logging
    logger = logging.getLogger(__name__)

    # Import checkpointer (lazy import to avoid circular dependencies)
    from app.core.langgraph_checkpoint import get_checkpointer
    from langgraph.prebuilt import create_react_agent

    # Create LLM with streaming enabled
    llm = create_llm()
    logger.info(f"Created LLM: {llm.model_name}, streaming={llm.streaming}")

    # â­ Use create_react_agent for proper streaming support
    # No tools needed for simple task decomposition
    checkpointer = get_checkpointer()
    logger.info(f"Got checkpointer: {checkpointer is not None}")

    # Create agent with system prompt (LangGraph 1.0.3 API)
    graph = create_react_agent(
        model=llm,
        tools=[],  # No tools for basic task decomposition
        checkpointer=checkpointer,
        prompt=SYSTEM_PROMPT,  # Pass as string, not SystemMessage
    )

    logger.info(f"Created react agent, graph type: {type(graph).__name__}")

    # Store system prompt as graph metadata for compatibility
    graph.system_prompt = SYSTEM_PROMPT

    return graph


# ============================================================================
# Public API - Global Instance
# ============================================================================

_graph_instance: Optional[Any] = None


def get_task_igniter_agent():
    """
    Get or create the global Task Igniter Graph instance.

    Lazy initialization pattern for efficiency.

    Returns:
        Compiled agent graph instance
    """
    global _graph_instance
    if _graph_instance is None:
        _graph_instance = create_task_igniter_graph()
    return _graph_instance


# ============================================================================
# Public API - Compatibility Functions
# ============================================================================

async def decompose_task_async(user_input: str, project_id: Optional[int] = None) -> str:
    """
    Asynchronously decompose a task using the agent.

    Provides compatibility with Agno version's API.

    Args:
        user_input: User's task description
        project_id: Optional project ID for context

    Returns:
        Agent's response as string
    """
    graph = get_task_igniter_agent()

    # Prepare messages (system prompt is already in state_modifier)
    context = f"é¡¹ç›®ID: {project_id}\n\n" if project_id else ""
    user_message = f"{context}{user_input}"

    messages = [HumanMessage(content=user_message)]

    # Invoke graph (non-streaming for simple response)
    result = await graph.ainvoke({"messages": messages})

    # Extract content from last message
    if result and "messages" in result:
        last_message = result["messages"][-1]
        return last_message.content

    return ""


def decompose_task_sync(user_input: str, project_id: Optional[int] = None) -> str:
    """
    Synchronously decompose a task using the agent.

    Note: This is a compatibility wrapper. LangGraph prefers async execution.

    Args:
        user_input: User's task description
        project_id: Optional project ID for context

    Returns:
        Agent's response as string
    """
    import asyncio

    # Run async function in sync context
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(decompose_task_async(user_input, project_id))


# ============================================================================
# Example Usage
# ============================================================================

if __name__ == "__main__":
    # Test agent
    import asyncio

    test_input = "å‡†å¤‡é¡¹ç›®æ¼”ç¤ºPPT"

    print("ðŸ¤– Task Igniter Agent (LangGraph 1.0) Test\n")
    print(f"Input: {test_input}\n")
    print("=" * 50)

    result = asyncio.run(decompose_task_async(test_input))
    print(result)
