"""
Main Graph - Deep Task Researcher ä¸»å›¾ã€‚

å®Œæ•´çš„ä»»åŠ¡åˆ†è§£ç ”ç©¶æµç¨‹ï¼š
1. clarify â†’ æ¾„æ¸…ç”¨æˆ·æ„å›¾
2. write_research_brief â†’ ç”Ÿæˆç ”ç©¶ç®€æŠ¥
3. research_supervisor â†’ å§”æ‰˜ç ”ç©¶ï¼ˆè°ƒç”¨ Supervisor å­å›¾ï¼‰
4. final_decomposition â†’ ç”Ÿæˆæœ€ç»ˆä»»åŠ¡åˆ†è§£
"""

from typing import Literal
from datetime import datetime
from langgraph.graph import StateGraph, END, START
from langgraph.types import Command
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from .state import DeepTaskState, ClarifyWithUser, ResearchBrief, TaskDecomposition
from .prompts import (
    clarify_with_user_instructions,
    transform_messages_into_research_topic_prompt,
    final_report_generation_prompt
)
from .supervisor_graph import create_supervisor_graph
from app.core.llm_factory import get_langchain_llm
from app.core.langgraph_checkpoint import get_checkpointer


# ==================================================================================
# è¾…åŠ©å‡½æ•°
# ==================================================================================

def get_today_str() -> str:
    """è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²"""
    now = datetime.now()
    return f"{now:%a} {now:%b} {now.day}, {now:%Y}"


def format_messages(messages) -> str:
    """æ ¼å¼åŒ–æ¶ˆæ¯å†å²ä¸ºæ–‡æœ¬"""
    formatted = []
    for msg in messages:
        if hasattr(msg, "type"):
            role = "User" if msg.type == "human" else "Assistant"
        else:
            role = "Unknown"

        content = msg.content if hasattr(msg, "content") else str(msg)
        formatted.append(f"{role}: {content}")

    return "\n".join(formatted)


def format_task_decomposition(decomposition: TaskDecomposition) -> str:
    """æ ¼å¼åŒ–ä»»åŠ¡åˆ†è§£ç»“æœä¸º Markdown"""
    output = f"# ğŸ“ {decomposition.main_task_title}\n\n"
    output += f"**æè¿°**: {decomposition.main_task_description}\n\n"
    output += "## ğŸ”¹ å­ä»»åŠ¡åˆ—è¡¨\n\n"

    for i, subtask in enumerate(decomposition.subtasks):
        # æ ‡è®°æœ€å°å¯è¡Œä»»åŠ¡
        if i == decomposition.minimum_viable_task_index:
            marker = "â­"
        else:
            marker = f"{i+1}."

        output += f"{marker} **{subtask.title}** (ä¼˜å…ˆçº§: {subtask.priority})\n"
        output += f"   {subtask.description}\n\n"

    return output


# ==================================================================================
# ä¸»å›¾èŠ‚ç‚¹
# ==================================================================================

async def clarify_node(
    state: DeepTaskState,
    config: RunnableConfig
) -> Command[Literal["write_research_brief", "__end__"]]:
    """
    æ¾„æ¸…èŠ‚ç‚¹ - æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…ç”¨æˆ·æ„å›¾ã€‚

    å¦‚æœéœ€è¦æ¾„æ¸…ï¼Œè¿”å›é—®é¢˜ç»™ç”¨æˆ·å¹¶ç»“æŸï¼›
    å¦åˆ™ç»§ç»­åˆ°ä¸‹ä¸€é˜¶æ®µã€‚
    """
    llm = get_langchain_llm()

    # ä½¿ç”¨ structured output + retry
    structured_llm = (
        llm
        .with_structured_output(ClarifyWithUser)
        .with_retry(stop_after_attempt=3)
    )

    # æ„å»º prompt
    prompt = clarify_with_user_instructions.format(
        messages=format_messages(state["messages"]),
        date=get_today_str()
    )

    result = await structured_llm.ainvoke([HumanMessage(content=prompt)], config=config)

    if result.need_clarification:
        # éœ€è¦æ¾„æ¸…ï¼Œè¿”å›é—®é¢˜ç»™ç”¨æˆ·
        return Command(
            goto=END,
            update={
                "needs_clarification": True,
                "messages": [AIMessage(content=result.question)]
            }
        )
    else:
        # æ— éœ€æ¾„æ¸…ï¼Œç»§ç»­ä¸‹ä¸€æ­¥
        return Command(
            goto="write_research_brief",
            update={
                "needs_clarification": False,
                "messages": [AIMessage(content=result.verification)]
            }
        )


async def write_research_brief(
    state: DeepTaskState,
    config: RunnableConfig
) -> Command[Literal["research_supervisor"]]:
    """
    ç”Ÿæˆç ”ç©¶ç®€æŠ¥ã€‚

    å°†ç”¨æˆ·æ¶ˆæ¯è½¬åŒ–ä¸ºè¯¦ç»†çš„ä»»åŠ¡åˆ†è§£ç ”ç©¶é—®é¢˜ã€‚
    """
    llm = get_langchain_llm()

    structured_llm = (
        llm
        .with_structured_output(ResearchBrief)
        .with_retry(stop_after_attempt=3)
    )

    prompt = transform_messages_into_research_topic_prompt.format(
        messages=format_messages(state["messages"]),
        date=get_today_str()
    )

    result = await structured_llm.ainvoke([HumanMessage(content=prompt)], config=config)

    return Command(
        goto="research_supervisor",
        update={"research_brief": result.research_brief}
    )


async def research_supervisor(
    state: DeepTaskState,
    config: RunnableConfig
) -> Command[Literal["final_decomposition"]]:
    """
    å§”æ‰˜ç»™ Supervisor å­å›¾æ‰§è¡Œç ”ç©¶ã€‚

    Supervisor ä¼šå°†ç ”ç©¶é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œå¹¶è¡Œæ‰§è¡Œã€‚
    """
    supervisor_graph = create_supervisor_graph()

    result = await supervisor_graph.ainvoke({
        "supervisor_messages": [],
        "research_brief": state["research_brief"],
        "notes": [],
        "research_iterations": 0
    }, config=config)

    return Command(
        goto="final_decomposition",
        update={
            "supervisor_messages": result["supervisor_messages"],
            "notes": result["notes"]
        }
    )


async def final_decomposition(
    state: DeepTaskState,
    config: RunnableConfig
) -> Command[Literal["__end__"]]:
    """
    æœ€ç»ˆä»»åŠ¡åˆ†è§£ã€‚

    åŸºäºç ”ç©¶ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä»»åŠ¡åˆ†è§£æ–¹æ¡ˆã€‚
    """
    llm = get_langchain_llm()

    structured_llm = (
        llm
        .with_structured_output(TaskDecomposition)
        .with_retry(stop_after_attempt=3)
    )

    # æ„å»º prompt
    # å°† final_report_generation_prompt é€‚é…ä¸ºä»»åŠ¡åˆ†è§£åœºæ™¯
    base_prompt = final_report_generation_prompt.format(
        research_brief=state["research_brief"],
        messages=format_messages(state["messages"]),
        date=get_today_str(),
        findings="\n\n".join(state["notes"]) if state["notes"] else "æš‚æ— ç ”ç©¶ç»“æœï¼Œè¯·åŸºäºå¸¸è¯†è¿›è¡Œä»»åŠ¡åˆ†è§£ã€‚"
    )

    # æ·»åŠ ä»»åŠ¡åˆ†è§£ç‰¹å®šæŒ‡ä»¤
    task_prompt = base_prompt + """

**ä»»åŠ¡åˆ†è§£è¦æ±‚**ï¼š

ä½ éœ€è¦åŸºäºç ”ç©¶ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„ä»»åŠ¡åˆ†è§£æ–¹æ¡ˆã€‚è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

{
  "main_task_title": "ä¸»ä»»åŠ¡æ ‡é¢˜ï¼ˆ15-30å­—ï¼‰",
  "main_task_description": "ä¸»ä»»åŠ¡æè¿°ï¼ˆè¯¦ç»†è¯´æ˜ä»»åŠ¡çš„ç›®æ ‡å’ŒèƒŒæ™¯ï¼‰",
  "subtasks": [
    {
      "title": "å­ä»»åŠ¡1æ ‡é¢˜",
      "description": "å­ä»»åŠ¡1æè¿°ï¼ˆ50å­—ä»¥å†…ï¼Œè¯´æ˜å…·ä½“è¦åšä»€ä¹ˆï¼‰",
      "priority": 3  // ä¼˜å…ˆçº§ 1-5ï¼Œæ•°å­—è¶Šå°è¶Šé‡è¦
    },
    // ... 3-5ä¸ªå­ä»»åŠ¡
  ],
  "minimum_viable_task_index": 0  // æœ€å°å¯è¡Œä»»åŠ¡çš„ç´¢å¼•ï¼ˆ0-basedï¼ŒæŒ‡å‘æœ€å®¹æ˜“å¼€å§‹çš„å­ä»»åŠ¡ï¼‰
}

**å­ä»»åŠ¡åˆ†è§£åŸåˆ™**ï¼š
1. æ¯ä¸ªå­ä»»åŠ¡è¦å…·ä½“ã€å¯æ‰§è¡Œã€æœ‰æ˜ç¡®çš„å®Œæˆæ ‡å‡†
2. ç¬¬ä¸€ä¸ªå­ä»»åŠ¡åº”è¯¥æ˜¯æœ€å®¹æ˜“å¼€å§‹çš„ï¼ˆé™ä½å¯åŠ¨æ‘©æ“¦ï¼‰
3. æŒ‰é€»è¾‘é¡ºåºæ’åˆ—å­ä»»åŠ¡ï¼ˆè€ƒè™‘ä¾èµ–å…³ç³»ï¼‰
4. æ¯ä¸ªå­ä»»åŠ¡æ ‡é¢˜15-30å­—ï¼Œæè¿°50å­—ä»¥å†…
5. ä¼˜å…ˆçº§ï¼š1=æœ€é«˜ä¼˜å…ˆçº§ï¼Œ5=æœ€ä½ä¼˜å…ˆçº§

**ä½¿ç”¨ç”¨æˆ·è¯­è¨€**ï¼š
- å¦‚æœç”¨æˆ·æ¶ˆæ¯æ˜¯ä¸­æ–‡ï¼Œæ‰€æœ‰è¾“å‡ºå¿…é¡»æ˜¯ä¸­æ–‡
- å¦‚æœç”¨æˆ·æ¶ˆæ¯æ˜¯è‹±æ–‡ï¼Œæ‰€æœ‰è¾“å‡ºå¿…é¡»æ˜¯è‹±æ–‡
"""

    try:
        result = await structured_llm.ainvoke([HumanMessage(content=task_prompt)], config=config)

        # æ ¼å¼åŒ–è¾“å‡º
        formatted_output = format_task_decomposition(result)

        return Command(
            goto=END,
            update={
                "final_output": formatted_output,
                "messages": [AIMessage(content=formatted_output)]
            }
        )

    except Exception as e:
        # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"ä»»åŠ¡åˆ†è§£å¤±è´¥: {str(e)}", exc_info=True)

        error_message = f"ä»»åŠ¡åˆ†è§£è¿‡ç¨‹é‡åˆ°é”™è¯¯: {str(e)}\n\nè¯·æ‰‹åŠ¨åˆ†è§£ä»»åŠ¡ï¼Œæˆ–é‡æ–°å°è¯•ã€‚"

        return Command(
            goto=END,
            update={
                "final_output": error_message,
                "messages": [AIMessage(content=error_message)]
            }
        )


# ==================================================================================
# åˆ›å»ºä¸»å›¾
# ==================================================================================

def create_deep_task_researcher():
    """
    åˆ›å»º Deep Task Researcher ä¸»å›¾ã€‚

    å®Œæ•´æµç¨‹ï¼š
    1. clarify â†’ æ¾„æ¸…ç”¨æˆ·æ„å›¾ï¼ˆå¯èƒ½æå‰ç»“æŸï¼‰
    2. write_research_brief â†’ ç”Ÿæˆç ”ç©¶ç®€æŠ¥
    3. research_supervisor â†’ å§”æ‰˜ç ”ç©¶ï¼ˆè°ƒç”¨ Supervisor å­å›¾ï¼‰
    4. final_decomposition â†’ ç”Ÿæˆæœ€ç»ˆä»»åŠ¡åˆ†è§£

    Returns:
        ç¼–è¯‘åçš„ StateGraphï¼ˆå¸¦ checkpointerï¼‰
    """
    workflow = StateGraph(DeepTaskState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("clarify", clarify_node)
    workflow.add_node("write_research_brief", write_research_brief)
    workflow.add_node("research_supervisor", research_supervisor)
    workflow.add_node("final_decomposition", final_decomposition)

    # å®šä¹‰æµç¨‹ï¼ˆCommand API è‡ªåŠ¨å¤„ç†è·¯ç”±ï¼‰
    workflow.add_edge(START, "clarify")

    # ç¼–è¯‘ï¼ˆå¸¦ checkpointer ç”¨äºæŒä¹…åŒ–ï¼‰
    checkpointer = get_checkpointer()
    if checkpointer:
        return workflow.compile(checkpointer=checkpointer)
    else:
        # åå¤‡æ–¹æ¡ˆï¼šæ—  checkpointer ç¼–è¯‘
        import logging
        logger = logging.getLogger(__name__)
        logger.warning("Checkpointer æœªåˆå§‹åŒ–ï¼ŒDeep Task Researcher å°†ä¸æ”¯æŒä¼šè¯æŒä¹…åŒ–")
        return workflow.compile()
